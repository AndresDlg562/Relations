[
    {
        "num_chapter": 0,
        "title": "Introducci\u00f3n y Presentaci\u00f3n de Joaqu\u00edn Bravo",
        "start_paragraph_number": 0,
        "end_paragraph_number": 11,
        "start_time": 1,
        "end_time": 219,
        "paragraphs": [
            "Alumnos de sexto semestre y tesistas de tequila en el campus Monterrey. ",
            "Gracias, \u00c1lvaro, por conectarte. Tambi\u00e9n quiero darle la bienvenida a Joaqu\u00edn Bravo. Joaqu\u00edn Bravo es programador con 20 a\u00f1os de experiencia. Trabaj\u00f3 en su propio despacho de software durante 10 a\u00f1os, desarrollando soluciones con Drupal para peri\u00f3dicos, revistas y universidades. Tambi\u00e9n ayud\u00f3 a organizar conferencias de c\u00f3digo abierto en Latinoam\u00e9rica, lo cual es muy padre. Despu\u00e9s estuvo 6 a\u00f1os en Wayland, donde empez\u00f3 como Tellez y termin\u00f3 como director de innovaci\u00f3n. Actualmente trabaja en Vicks, desarrollando soluciones de all\u00e1. Es un experto en el \u00e1rea, y me da mucho gusto que podamos tenerlo aqu\u00ed, en este espacio, en el sal\u00f3n, compartiendo sus conocimientos con nosotros. Joaqu\u00edn, muchas gracias. Bienvenido.",
            "Gracias, Elvia, por invitarme otra vez a estar aqu\u00ed. El semestre pasado, Joaqu\u00edn nos acompa\u00f1\u00f3 tambi\u00e9n en el sal\u00f3n. Gracias por volver a repetirlo, Joaqu\u00edn. Con mucho gusto. ",
            "Pueden ver mi pantalla. \u00bfSabes? Perd\u00ed la conexi\u00f3n un poco. D\u00e9jame volver a conectar mientras muevo la computadora. Ha estado fallando. ",
            "\u00bfMe escuchan? Bueno, s\u00ed, te escuchamos. Si quieren irse conectando por Teams, tambi\u00e9n pueden hacerlo, por si no logro conectar aqu\u00ed la pantalla. Solo necesito que todos en mi otra llamada se mantengan en silencio para que no se vicie el micr\u00f3fono. ",
            "\u00bfAh, s\u00ed? Pero s\u00ed se pueden desmontar en cualquier momento si tienen dudas, con toda confianza. ",
            "Hola, buenos d\u00edas. ",
            "Hola, hola. No creo que aprend\u00ed. Ah\u00ed est\u00e1, ya estamos. Ahora, s\u00ed, d\u00e9jame ver si conecta mi laptop. Hola, hola. ",
            "S\u00ed, no. Ya hab\u00eda funcionado, pero mientras mov\u00eda la computadora, como que dej\u00f3 de funcionar. Siento que est\u00e1 fallando, m\u00e1s bien es el cable de HDMI, \u00bfno? A veces hay que revisar el audio para ver a d\u00f3nde se est\u00e1 mandando. ",
            "S\u00ed, pero no hay problema. Ni siquiera est\u00e1 recibiendo la se\u00f1al. No, no, no puedo acceder. Pero creo que s\u00ed te escuchamos. No s\u00e9 si alcanzan a escuchar hasta atr\u00e1s. ",
            "S\u00ed, s\u00ed, y nos conectamos por Teams. Entonces, adelante, Joaqu\u00edn. Ustedes me dicen. "
        ],
        "paragraph_timestamps": [
            1,
            11,
            53,
            69,
            83,
            97,
            104,
            134,
            167,
            201,
            214
        ]
    },
    {
        "num_chapter": 1,
        "title": "Desarrollo de Soluciones con Inteligencia Artificial",
        "start_paragraph_number": 11,
        "end_paragraph_number": 20,
        "start_time": 219,
        "end_time": 448,
        "paragraphs": [
            "Bueno, esta presentaci\u00f3n es como una introducci\u00f3n a c\u00f3mo desarrollar soluciones con inteligencia artificial. Hay un t\u00e9rmino que est\u00e1 muy de moda, que es el de ingenieros de IA, que creo que es una combinaci\u00f3n de cosas que ustedes ya saben y que est\u00e1n aprendiendo como ingenieros en sistemas, m\u00e1s algunas otras cosas. ",
            "Es un poquito de lo que vamos a estar hablando, pero siempre tenemos que estar muy enfocados en construir soluciones, no tanto en la tecnolog\u00eda, sino al final en el producto que queremos hacer. ",
            "\u00bfYa me present\u00f3 Elvia? Entonces, soy ingeniero, y s\u00ed, es un poquito diferente a otros roles ya establecidos en la industria, como un data scientist o un machine learning engineer. En el sentido de que, por ejemplo, un engineer no est\u00e1 construyendo o desarrollando nuevos modelos, ni siquiera est\u00e1 haciendo reentrenamiento de modelos existentes. ",
            "No requiere, al menos para esta parte de construir soluciones con inteligencia artificial, de teor\u00eda muy avanzada sobre inteligencia artificial, redes neuronales o matem\u00e1ticas. No se enfoca en ese estilo, sino simplemente en conocer muy bien los modelos y las herramientas que se pueden utilizar para crear soluciones aumentadas, en ingl\u00e9s, con inteligencia artificial. ",
            "Mucho de esta presentaci\u00f3n, adem\u00e1s, ya hay, o sea, como el t\u00e9rmino est\u00e1 muy de moda, a lo mejor ya han visto esta p\u00e1gina de Roach, que es muy buena, y justo acaban de estrenar este nuevo de Einer, que trae muchos t\u00e9rminos y buenos links para profundizar m\u00e1s. ",
            "A grandes rasgos, podemos decir que cuando vamos a construir este tipo de soluciones, vamos a tener ciertos binding blocks, ciertas piezas de lego que vamos a usar para construir estas soluciones. La primera, muy obvia, es esta herramienta, que es como ChatGPT, que nos vino a revolucionar toda esta industria y acelerar todo por ah\u00ed a finales del 2022. ",
            "Esa es una herramienta, los prompts, que es la manera en la que vamos a estar interactuando principalmente con estos elementos. El conocimiento que pueden obtener estos elementos no es solo el que ya tienen porque ya fueron entrenados por alguien m\u00e1s, sino el que les vamos a dar nosotros de nuestros usuarios y de nuestra tecnolog\u00eda para mejorar la aplicaci\u00f3n. ",
            "Tambi\u00e9n hay integraciones con otros servicios, y vamos a ver que hay muchos otros modelos de los que podemos servir. No todos son modelos de lenguaje. Hay muchos otros modelos que, como ingenieros, es muy bueno conocerlos para no centrarnos \u00fanicamente en matar mosquitos a escopetazos. ",
            "Adem\u00e1s, es importante saber c\u00f3mo implementar algo de integraci\u00f3n continua y despliegue continuo en nuestras soluciones para que sean robustas y funcionen bien en producci\u00f3n. Esto tambi\u00e9n est\u00e1 basado en otra presentaci\u00f3n interesante de Look March\u00e1n, que se titula \"Todos podemos ser ingenieros\". Esto se basa en el conocimiento que ya tienen. Es decir, si ustedes ya saben lenguajes de programaci\u00f3n, librer\u00edas, bases de datos e integraciones con servicios, ya tienen mucho de lo que se necesita."
        ],
        "paragraph_timestamps": [
            219,
            240,
            259,
            286,
            318,
            338,
            366,
            389,
            417
        ]
    },
    {
        "num_chapter": 2,
        "title": "Componentes y Evaluaci\u00f3n de Modelos de IA",
        "start_paragraph_number": 20,
        "end_paragraph_number": 26,
        "start_time": 448,
        "end_time": 682,
        "paragraphs": [
            "Ahora, vamos a ver algunos de estos componentes, estas piezas del ecosistema. La primera, la m\u00e1s importante, es donde vamos a pasar m\u00e1s tiempo, en la parte de LLMs. Tenemos que escoger entre dos grandes grupos: los LLMs en la nube y los LLMs de c\u00f3digo abierto. En realidad, esa l\u00ednea es un poco difusa, pero es \u00fatil para entender mejor las variaciones que vamos a ver entre los LLMs disponibles para nosotros. Estas variaciones vienen dadas por la calidad del modelo, es decir, qu\u00e9 tan buenas respuestas da y qu\u00e9 tanto conocimiento tiene.",
            "Otro aspecto importante es el tama\u00f1o del contexto, o \"context window\", que es fundamental conocer. Tambi\u00e9n debemos considerar el precio: cu\u00e1nto nos va a costar hospedarlo, ya sea en nuestros propios equipos o usando un servicio en la nube. Hay muchas maneras de evaluar esto, y hay muchos recursos disponibles, como tablas y gr\u00e1ficos en donde podemos ver c\u00f3mo est\u00e1n evaluados los diferentes modelos. ",
            "Por ejemplo, hay un sitio llamado artificialanalysis.ai donde siempre est\u00e1n comparando modelos de inteligencia artificial. Se puede hacer un \"drill down\" de las pruebas que est\u00e1n realizando. Actualmente, no est\u00e1 actualizado con el \u00faltimo modelo que sali\u00f3 esta semana, que es el Claude 3.7. De hecho, s\u00ed est\u00e1 disponible, y lo actualizan muy r\u00e1pido.",
            "En cuanto a la velocidad de respuesta, esto tambi\u00e9n es importante a considerar dependiendo del tipo de aplicaci\u00f3n que estemos usando. Por ejemplo, en la parte superior de las evaluaciones est\u00e1 el modelo \"Owned Mini\". El precio tambi\u00e9n es un factor a tener en cuenta. Muchos modelos tienen una opci\u00f3n gratuita, pero cuando quieres lanzar algo en producci\u00f3n, esa opci\u00f3n probablemente no ser\u00e1 suficiente, y el precio comenzar\u00e1 a ser importante.",
            "Algo m\u00e1s que quer\u00eda mencionar sobre el precio es que estas son evaluaciones de otros modelos, y conviene familiarizarse con algunas de ellas. Aqu\u00ed, por ejemplo, explican exactamente qu\u00e9 evaluaci\u00f3n est\u00e1n usando, como la de Nigma, el Multi Challenge o la de Visual Understanding. Dependiendo del tipo de soluci\u00f3n que queremos hacer, si tiene mucho de visual, tal vez nos convenga usar Gemini en lugar de otro modelo.",
            "Esta otra opci\u00f3n es un repositorio de GitHub donde se puede ver c\u00f3mo est\u00e1 hecho este sitio, lstats.stats.com. Ellos traen una historia de c\u00f3mo han ido evolucionando algunos modelos con el tiempo. Podemos ver c\u00f3mo han ido subiendo en sus puntuaciones. En la parte superior, podemos ver a Brock y OpenAI, mientras que m\u00e1s abajo est\u00e1 Mistral, que es una empresa francesa. Ellos tienen m\u00e1s evaluaciones sobre cu\u00e1l es el mejor para c\u00f3digo y razonamiento."
        ],
        "paragraph_timestamps": [
            448,
            491,
            524,
            558,
            608,
            634
        ]
    },
    {
        "num_chapter": 3,
        "title": "Introducci\u00f3n a Modelos de Lenguaje y Tama\u00f1o de Contexto",
        "start_paragraph_number": 26,
        "end_paragraph_number": 31,
        "start_time": 682,
        "end_time": 872,
        "paragraphs": [
            "El tama\u00f1o del contexto ya lo mencionamos brevemente, pero el campe\u00f3n indiscutible en este momento es el modelo de Google, que tiene un tama\u00f1o de contexto de 2,000,000 de tokens. Cuando necesitamos enviar mucha informaci\u00f3n, este modelo ser\u00e1 de las mejores opciones, junto con otros factores como el precio y la velocidad.",
            "Parece que ya logr\u00e9 conectar el sal\u00f3n. D\u00e9jame cambiar la bocina un segundo. \u00bfMe escuchan bien? Hola, hola, \u00bfme escuchan? S\u00ed, los escuchamos.",
            "Muy bien, esta es otra gr\u00e1fica que creo que es de las \u00faltimas que les quer\u00eda mostrar. La pongo ahorita; est\u00e1 comparando otra vez el score, ahora contra el precio. Entonces, est\u00e1 nuevamente a la hora de que vamos a salir. A producci\u00f3n va a ser muy relevante, \u00bfno? Mientras m\u00e1s para ac\u00e1, mejor calidad de las respuestas, y mientras m\u00e1s arriba son, m\u00e1s caros, \u00bfno? Entonces, estos que est\u00e1n abajo, pues tienen un s\u00faper precio. La versi\u00f3n mini tambi\u00e9n tiene muy buen precio, y Claud Sweet, pues es un poquito m\u00e1s caro, pero tambi\u00e9n ahorita es el que tiene la corona del que entiende la mejor calidad de respuestas. ",
            "Incluso este de Sonnet, este de Claud, en la parte de c\u00f3digo es s\u00faper relevante, ya que tiene una calificaci\u00f3n muy por encima de muchos otros modelos. Estaba escuchando de parte de la comunidad que desarrolla este tipo de soluciones que est\u00e1n impresionados, \u00bfno?, de la calidad. Adem\u00e1s, esto es porque junto con el nuevo modelo, liberaron un software Open Source que puedes usar en tu computadora para ayudarte a codificar, que es como una gente, \u00bfno?, del c\u00f3digo. ",
            "Entonces, eso lo hace a\u00fan m\u00e1s relevante. Hablamos hasta ahorita casi de puros modelos hospedados en la nube. Si se fijan, por ah\u00ed hab\u00eda algunos Open Source. De hecho, por ejemplo, de Ipsec lo puedes, lo Torres, normalmente en la nube, porque de hecho correrlo en Python en tu propia computadora no es tan f\u00e1cil. Para empezar, por el tama\u00f1o del modelo, que es un modelo muy grande, no como los modelos de Lama, que hay versiones m\u00e1s chiquitas. S\u00ed, es enorme y no es factible correrlo; m\u00e1s que versiones ya muy reducidas que no tienen la misma calidad de respuestas."
        ],
        "paragraph_timestamps": [
            682,
            739,
            759,
            802,
            838
        ]
    },
    {
        "num_chapter": 4,
        "title": "Privacidad y Soberan\u00eda en Modelos Open Source",
        "start_paragraph_number": 31,
        "end_paragraph_number": 38,
        "start_time": 872,
        "end_time": 1185,
        "paragraphs": [
            "\u00bfEntonces estamos hablando de Claud models? Ahora, \u00bfpor qu\u00e9 querr\u00eda uno usar o por qu\u00e9 considerar de cualquier manera modelos Open Source? Bueno, hay varias razones muy buenas. La primera que luego sale y que pesa mucho es la privacidad de la informaci\u00f3n. Si vamos a estar tratando con datos privados de nuestra empresa que no quieren que se est\u00e9n mandando a proveedores terceros, entonces la \u00fanica opci\u00f3n que queda es correr modelos Open Source, que puedes correr en tus propios equipos, en tus propios servidores, en tu propia LAN. As\u00ed evitas cualquier problema de privacidad o de seguridad de la informaci\u00f3n.",
            "Muchas veces, el Gobierno de Estados Unidos, por ejemplo, tiene ese tipo de pol\u00edticas. Seguramente muchos bancos y muchos clientes van a tener ese tipo de restricciones, a pesar de que muchos Claud providers, como el mismo OpenAI, Concha, GPT, Claud, todos ellos te dicen que si est\u00e1s usando la versi\u00f3n free, pues nada es gratis en esta vida. Al usarlo gratis, lo que te est\u00e1n cobrando es que van a usar los datos que t\u00fa les mandes para seguir entrenando sus modelos. Cuando pagas por sus APIs, ellos te dicen que no, que en la licencia, justo por pagar por sus APIs, ya no van a usar esos datos para entrenar sus modelos. Pero a\u00fan con eso, con esa promesa, no hay nada que te la asegure. Con esa promesa de los proveedores cloud, de todas formas, muchos clientes optan por no utilizar estos modelos y m\u00e1s bien pulsar modelos que puedas correr en tu propia computadora, en tus propios servidores.",
            "Entonces, la privacidad de la informaci\u00f3n es muy importante. Muy relacionado a ese t\u00e9rmino est\u00e1 la parte de soberan\u00eda, no soberan\u00eda de la informaci\u00f3n y de la tecnolog\u00eda. Ah\u00ed podemos ver, por ejemplo, c\u00f3mo famosamente Estados Unidos, en su Gobierno, no va a querer que usen modelos chinos. Los chinos seguramente tambi\u00e9n tienen restricciones similares en donde, por nada del mundo, van a estar usando modelos gringos. Los europeos invierten tambi\u00e9n fuertemente en modelos propios, como esta compa\u00f1\u00eda de Mistral. Israel tiene sus propios modelos, y as\u00ed. Esta parte de soberan\u00eda de la informaci\u00f3n es no depender de fuerzas externas, tener control sobre la tecnolog\u00eda y la informaci\u00f3n que manejas, es un concepto muy importante. ",
            "No solo aplica a pa\u00edses, tambi\u00e9n puede aplicar a empresas. Esto est\u00e1 muy relacionado con la privacidad, pero la soberan\u00eda tambi\u00e9n te puede orientar a usar modelos Open Source, que adem\u00e1s van a tener m\u00e1s transparencia. Puedes tener m\u00e1s confianza en ellos, porque si tienes alguna duda de por qu\u00e9 se est\u00e1 comportando de cierta manera, los puedes ver. No son Open Source y, por eso, al ser abiertos, puedes ver c\u00f3mo es que las respuestas se est\u00e1n generando de cierta manera. ",
            "Algo que hay que mencionar es qu\u00e9 tan Open Source realmente es el modelo, \u00bfno? \u00bfQu\u00e9 tan abierto realmente es? Porque hay tres cosas que pueden abrir. Una cosa es abrir los pesos del modelo, que es como quien dice que puedas usar el ejecutable, el bajarte el punto exe y usarlo. Eso depende de la licencia que le pongan, y muchas licencias de estos modelos son licencias abiertas, que puedes usar para lo que gustes. Algunas tienen licencias no comerciales. Algunos de estos modelos los puedes usar, pero para usos m\u00e1s bien educativos, acad\u00e9micos o de investigaci\u00f3n, pero no para uso comercial. Hay que revisar la licencia, pero eso de liberar los pesos es, es, es como liberarte una estable. No, realmente no hay una transparencia sobre c\u00f3mo funciona el modelo.",
            "Algunos liberan tambi\u00e9n el dataset, que es lo m\u00e1s raro; en realidad, muy poco liberal en el dataset, pero s\u00ed hay algunos datasets libres que puedes ver, o pueden liberar el c\u00f3digo. El c\u00f3digo necesario tambi\u00e9n para, ya sea que usaron para entrenarlo o el que se usa para correrlo, \u00bfno? Y bueno, dentro de estos niveles de transparencia es donde vas a ir ganando confianza tambi\u00e9n en el modelo. Con esta transparencia y con esto que podemos ver del modelo, de c\u00f3mo funciona, pues tambi\u00e9n nos permite customizarlo de mejor manera. Es m\u00e1s f\u00e1cil customizar modelos Open Source, que puedes bajar el modelo y entrenarlo en tu propio equipo, que hacer este tipo de customizaciones contra modelos como los de OpenAI y otros proveedores.",
            "Siempre se habla primero de OpenAI porque es el primero y es de donde muchos se est\u00e1n copiando muchas de estas buenas pr\u00e1cticas. Te ofrece de manera de hacer fine-tuning de su modelo, pero es dif\u00edcil, es costoso y te est\u00e1s atando a ese modelo. Mientras que si lo haces con un modelo Open Source, tienes un poco m\u00e1s de libertad hacia donde moverte. Te est\u00e1s dejando de depender de un proveedor externo."
        ],
        "paragraph_timestamps": [
            872,
            916,
            975,
            1021,
            1051,
            1102,
            1154
        ]
    },
    {
        "num_chapter": 5,
        "title": "Ventajas de Usar Modelos Open Source",
        "start_paragraph_number": 38,
        "end_paragraph_number": 41,
        "start_time": 1185,
        "end_time": 1298,
        "paragraphs": [
            "Finalmente, otra parte que se menciona aqu\u00ed es usar Open Source para ahorrarnos dinero. Bueno, con todo lo que han sido abaratados los costos. Les mostraba yo esta tablita anterior, donde los m\u00e1s baratos realmente son OpenAI con GPT-3.5, que es s\u00faper barato. Claude de Anthropic tampoco es tan caro. A Jiminy, no s\u00e9 por qu\u00e9, pero es barat\u00edsimo; realmente es rid\u00edculo lo que cuesta el token. Entonces, a veces no tanto, vamos a ahorrar en el costo. S\u00ed, pueden ser mucho m\u00e1s eficientes algunos de estos modelos en la nube, pero est\u00e1 tambi\u00e9n este componente en donde un modelo Open Source puede llegar a correr incluso en tu computadora.",
            "Hay modelos Open Source que puedo correr en mi computadora personal, en una MacBook Pro, por ejemplo, o en una computadora que tenga estos \u00faltimos chips que est\u00e1 sacando Intel o AMD, que tienen soporte para inteligencia artificial. Con cierto RAM, pueden correr en esos, o incluso en celulares. En el edge, le llaman. Hay modelos tan peque\u00f1os que pueden correr tambi\u00e9n ya en tu celular o en dispositivos peque\u00f1os como Raspberry Pi, por ejemplo. Entonces, ciertamente puede haber tambi\u00e9n un ahorro en costos, puesto que no est\u00e1s necesitando ni siquiera correrlo en un equipo que ya tienes.",
            "Aqu\u00ed tambi\u00e9n puede haber cierto ahorro a la ecolog\u00eda. Adem\u00e1s, estos modelos, como se sabe, c\u00f3mo funcionan los modelos Open Source, son los que han avanzado mucho m\u00e1s en esta parte de la eficiencia y del ahorro en qu\u00e9 tan caro es correrlos. Son los que han empujado 100% a que bajen los precios tan dram\u00e1ticamente los modelos en la nube. Entonces, s\u00ed vale la pena considerar eso tambi\u00e9n."
        ],
        "paragraph_timestamps": [
            1185,
            1228,
            1266
        ]
    },
    {
        "num_chapter": 6,
        "title": "Evaluaci\u00f3n de Modelos Open Source",
        "start_paragraph_number": 41,
        "end_paragraph_number": 47,
        "start_time": 1298,
        "end_time": 1441,
        "paragraphs": [
            "Entrando ya a evaluaciones de modelos Open Source, est\u00e1 este, por ejemplo, el leaderboard de Hugging Face. Para estos screenshots, solo list\u00e9 los proveedores oficiales como para alistar aqu\u00ed los m\u00e1s famosos, justo para que salga. Por ejemplo, cuenta que es un modelo chino, de los mejores modelos que puedes correr justo en hardware relativamente que no necesitas un supercomputador. Est\u00e1 Mistral, este modelo europeo que les llamaba la AMA, obviamente de Meta. ",
            "Entonces, estos modelos los podemos ver en este listado. Hay otros filtros en donde si dices t\u00fa: \"Oye, \u00bfcu\u00e1l puedo correr en un consumer device? \u00bfNo? \u00bfEn una PC? \u00bfCu\u00e1l puedo correr en un celular?\" Y si se fijan, lo que baja principalmente es el n\u00famero de par\u00e1metros. Estos modelos cuentan con 72 billones de par\u00e1metros. Vic tiene 600 billones de par\u00e1metros. En realidad, no es factible correrlo m\u00e1s que en un supercomputador. ",
            "Estos modelos, para que puedan correr en computadoras personales, tienen 3.5. Esta es la versi\u00f3n a 3 billones de par\u00e1metros, a 6 billones de par\u00e1metros, y corren en una computadora personal. ",
            "Si nos vamos a los que son para edge, no tengo aqu\u00ed abierta la ventana, pero seguramente nos van a salir modelos de un bill\u00f3n de par\u00e1metros, m\u00e1ximo dos billones de par\u00e1metros. Y esos s\u00ed que son modelos que podr\u00e1s correr tambi\u00e9n hasta en tu celular muchas veces. Ah, hay por ah\u00ed, creo que el link lo ten\u00eda por ac\u00e1, donde lo dej\u00e9. ",
            "Ah, s\u00ed, este. Bueno, tambi\u00e9n la pregunta que luego sale es porque no sale de ipsec, \u00bfno? En estos s\u00ed, si es de los m\u00e1s chidos Open Source, pues bueno, ah\u00ed cierta especulaci\u00f3n. En el mismo foro, Foro de Joaqu\u00edn FACE, en donde, seguramente una parte es porque es un modelo de 600 billones de par\u00e1metros. Es grand\u00edsimo y los modelos que salen en esa tabla llegan hasta 140,000,000 de par\u00e1metros. O tal vez tambi\u00e9n porque dic todav\u00eda no corre con la librer\u00eda de Transformers, que es una de las librer\u00edas Open Source m\u00e1s usadas. ",
            "\u00bfSino es que la m\u00e1s usada para correr este tipo de modelos, no? Bueno, entonces, si tenemos alguna de estas caracter\u00edsticas que nos dice, creo que un modelo Open Source ser\u00eda una buena idea. \u00bfC\u00f3mo los corremos? "
        ],
        "paragraph_timestamps": [
            1298,
            1328,
            1353,
            1369,
            1392,
            1424
        ]
    },
    {
        "num_chapter": 7,
        "title": "Implementaci\u00f3n y Ejecuci\u00f3n de Modelos Locales",
        "start_paragraph_number": 47,
        "end_paragraph_number": 55,
        "start_time": 1441,
        "end_time": 1656,
        "paragraphs": [
            "Lo m\u00e1s sencillo y por donde les dir\u00eda que comenzar es por este software de O Lama. Por detr\u00e1s, usa la m\u00e1s P o otro proyecto en C++ que corre. No puede correr en GPU, pero tambi\u00e9n corre en CPU, es decir, no necesitas una super tarjeta gr\u00e1fica para correr O Lama o para correr algunos de estos modelos. Entonces, esa es la manera m\u00e1s sencilla. Es como el Docker de los L Tnes. Yo lo tengo instalado en mi computadora, tengo varios modelos ah\u00ed y es muy eficiente. ",
            "Yolanda, despu\u00e9s tambi\u00e9n lo podr\u00edas correr en producci\u00f3n en alg\u00fan servidor Linux. Podr\u00edas correr O Lama y estar corriendo ah\u00ed tus modelos ya para sistemas en producci\u00f3n. Correrlo tal cual sobre Python tambi\u00e9n es una opci\u00f3n porque permite ciertas optimizaciones que tal vez O Lama no soporta para CUDA. Por ejemplo, hay un proyecto que se llama BLM que es muy eficiente para servir a millones de clientes, desde Python o para la Mac. ",
            "Hay cierto c\u00f3digo de Python que utiliza bien los procesadores de Mac optimizados para inteligencia artificial, espec\u00edficamente la arquitectura MLX, que O Lama hoy en d\u00eda no lo soporta. Hay un ticket abierto por ah\u00ed, pero no han agregado esa funcionalidad. Entonces, hay ciertos casos en donde conviene correr este c\u00f3digo de Python. ",
            "Ah, o si est\u00e1. Nada m\u00e1s probando, hay uno que se llama El Estudio. Este no es Open Source, O Lama s\u00ed es Open Source. El Estudio no es Open Source, pero tambi\u00e9n si quieren jugar con modelos en su computadora, esta es una buena soluci\u00f3n que s\u00ed corre esta arquitectura de MLX con Python en la mano. Entonces, tambi\u00e9n vale la pena considerarlo para jugar, no para producci\u00f3n. ",
            "Y hay algunos estimadores de qu\u00e9 tanto RAM necesitar\u00edas para correr un modelo de manera local en tu computadora. Por ejemplo, aqu\u00ed te est\u00e1 diciendo que tengo un modelo de 25,000,000 de billones de par\u00e1metros que est\u00e1 cuantizado. Cuantizado es una palabra que usan para decir que los modelos normalmente est\u00e1n basados en arreglos de vectores que tienen 16 bits y luego le reducen el n\u00famero de bits en cada uno de esos vectores para reducir el tama\u00f1o del modelo. ",
            "Sigue teniendo 25,000,000 de par\u00e1metros, pero en n\u00fameros m\u00e1s chiquitos que los hacen m\u00e1s eficientes. Si bien pierden, claro, bastante de la calidad en la respuesta, no toda. O sea, no se vuelven tontos, pero no tienen la misma calidad en las respuestas. Entonces, un modelo de estos de 25 billones de par\u00e1metros cuantizado a cuatro bits se estima que pudiera correr, por ejemplo, con una computadora que tenga 16 GB de RAM. ",
            "As\u00ed que podemos ver que si bajamos los n\u00fameros de par\u00e1metros a algo m\u00e1s chiquito, por ejemplo, yo tengo 8 gigas de RAM. \u00bfPuedo correr? Tal vez hasta 9 billones de par\u00e1metros con una cuantizaci\u00f3n de cuatro bits. Este fue el primero que encontr\u00e9 en Internet. Hay otras calculadoras, ya no me gust\u00f3 esta, que no pude m\u00e1s bien modificar yo los par\u00e1metros y que me diga el RAM. ",
            "\u00bfEl RAM te dice cu\u00e1l modelo estima que vas a poder correr? Hay m\u00e1s, hay m\u00e1s calculadoras y nos da una idea general. Esta computadora que tengo yo aqu\u00ed tiene 36 gigas en RAM. Entonces, puedo poner modelos m\u00e1s grandes. "
        ],
        "paragraph_timestamps": [
            1441,
            1468,
            1501,
            1527,
            1553,
            1585,
            1610,
            1640
        ]
    },
    {
        "num_chapter": 8,
        "title": "Integrating Large and Small Models",
        "start_paragraph_number": 55,
        "end_paragraph_number": 63,
        "start_time": 1656,
        "end_time": 1913,
        "paragraphs": [
            "Finalmente, esta frase que me gusta mucho de este comercial: \u00bfpor qu\u00e9 no los dos? Acaban de anunciar tambi\u00e9n esta semana un proyecto en colaboraci\u00f3n, me parece que es la Universidad de Stanford, junto con Obama, en donde est\u00e1n haciendo proyectos en los que corren, por un lado, un modelo muy grande, remoto, como ChatGPT o como Claude, junto con modelos chiquitos locales de 8,000,000 de par\u00e1metros como O Lama. ",
            "Juntos pueden resolver problemas a una fracci\u00f3n del costo que si se lo mandaras nada m\u00e1s al modelo grande. El costo de resolver el problema es muy, muy alto, y la calidad de la respuesta es la mejor. Pero si usas esta combinaci\u00f3n de un modelo grandote orquestando muchos modelos chiquitos para cosas m\u00e1s sencillas, obtienes una muy buena calidad de respuesta a una fracci\u00f3n del costo.",
            "Entonces, este tipo de soluciones para algunos problemas, obviamente, no funciona para todo. Es un nicho muy espec\u00edfico de problemas, por lo que vale la pena investigar este tipo de soluciones. No solo se trata de usar ambos modelos, sino que algo m\u00e1s que les dir\u00eda es que intenten desacoplar, de todas formas, al principio que est\u00e1n desarrollando una soluci\u00f3n. Desacoplar no significa casarse por completo con alg\u00fan modelo, puesto que este est\u00e1 evolucionando constantemente. Es bueno usar librer\u00edas; hay libros para empezar. La mayor\u00eda de muchos de los proveedores soportan el mismo API que OpenAI.",
            "Por ejemplo, el mismo cliente que est\u00e1s usando en Python o en JavaScript de OpenAI para conectarte a la API, si le cambias el servidor y en vez de apuntar a OpenAI, apuntas a tu local host corriendo, va a funcionar. Entonces, hay mucha interoperabilidad. Por un principio, conviene no casarse tanto con un modelo espec\u00edfico. Me gust\u00f3 much\u00edsimo la idea de no casarse con el API de OpenAI, sino usar herramientas que te atraigan un poquito.",
            "OpenAI y su cliente son una de estas herramientas. Otras en Python son algunas librer\u00edas como Little Lem, que es una sola librer\u00eda con la que te conectas a todos los que quieras. Esta librer\u00eda soporta much\u00edsimos proveedores. Hay otra de un compa\u00f1ero llamado Andrew en G, que es famoso porque tiene un sitio de \"AI for Everyone\" y ofrecen cursos gratuitos de inteligencia artificial. Su equipo liber\u00f3 una librer\u00eda que se llama AI Suite. Tambi\u00e9n hay ejemplos de esa librer\u00eda que es multi.",
            "En JavaScript, tambi\u00e9n hay librer\u00edas como Lem 10. Ross tiene las suyas. Hay otra de una persona llamada Jason Lee que se llama Instructor, que es muy interesante. Esa misma librer\u00eda tiene soporte para Python, TypeScript, PHP, Ruby, Elixir, y seguramente con distintos niveles de soporte. En realidad, Python casi siempre es lo m\u00e1s importante en este tipo de soluciones. En segundo lugar, dir\u00eda que JavaScript es relevante.",
            "Una opini\u00f3n que tengo, que puede ser un poco controversial, es que siento que es mejor evitar, al menos en un principio, o sobre todo ustedes que est\u00e1n aprendiendo a desarrollar este tipo de soluciones, librer\u00edas grand\u00edsimas que tratan de abarcarlo todo, como LangChain o como LlamaIndex. Se convierten en una capa de abstracci\u00f3n tan grande que te conviertes m\u00e1s en un ingeniero de LangChain o de LlamaIndex y en realidad no aprendes el detalle fino de algunos de los componentes de estas soluciones. ",
            "El lenguaje es un componente muy importante que puede tener su propia capa de abstracci\u00f3n, pero hay muchas otras partes de estas soluciones. Son bloques de Lego que les comentaba. Si todo lo est\u00e1s usando con LangChain, creo que al final m\u00e1s bien te va a limitar, adem\u00e1s de que tiene su propia curva de aprendizaje muy grande. Esa es mi opini\u00f3n personal, y es una opini\u00f3n muy compartida tambi\u00e9n en algunos foros."
        ],
        "paragraph_timestamps": [
            1656,
            1688,
            1717,
            1761,
            1789,
            1823,
            1850,
            1890
        ]
    },
    {
        "num_chapter": 9,
        "title": "Knowledge Management in AI Solutions",
        "start_paragraph_number": 63,
        "end_paragraph_number": 71,
        "start_time": 1913,
        "end_time": 2177,
        "paragraphs": [
            "Ahora, pasemos al siguiente bloque de Lego, que es el conocimiento. Bueno, los modelos ya tienen conocimiento, pero \u00bfc\u00f3mo doy conocimiento que ellos no tienen porque es propio de mi compa\u00f1\u00eda, de mi organizaci\u00f3n? Lo m\u00e1s normal es, por un lado, pensar en el fine-tuning. Pero el fine-tuning es muy caro. En realidad, la soluci\u00f3n por defecto que m\u00e1s se usa es aumentar justo el conocimiento que ya tiene, haciendo copy-paste, como quien dice, en el prompt de la informaci\u00f3n a la que tiene acceso el modelo para lograr hacer esto. ",
            "Para ello, necesitamos una base de datos. Esta es una presentaci\u00f3n m\u00e1s larga que tengo, nada m\u00e1s con un resumen de sistemas RAG, en donde normalmente tienes dos pasos. Por un lado, en tu base de datos, tienes toda la informaci\u00f3n que t\u00fa tengas de tu compa\u00f1\u00eda, que bien puede ser las \u00f3rdenes de compra, documentos en PDF o lo que sea. La indexas para que pueda ser buscada cuando el usuario est\u00e9 interactuando.",
            "En un primer momento, indexas toda esta informaci\u00f3n para las b\u00fasquedas, y cuando el usuario est\u00e9 interactuando, ya sea haciendo una pregunta directamente en un chat o interactuando con alg\u00fan filtro, vas a poder convertir esa interacci\u00f3n del usuario en un vector o algo que puedas buscar f\u00e1cilmente en esta base de datos que tienes de toda tu informaci\u00f3n. As\u00ed, puedes traerte el contenido relevante de la base de datos y envi\u00e1rselo al usuario para dar una respuesta.",
            "Este tipo de soluciones RAG son s\u00faper t\u00edpicas. En aprender m\u00e1s o menos c\u00f3mo funciona, \u00bfno? Pero a grandes rasgos, estos son todos sus componentes. No es una base de datos de vectores que bien puede ser el mismo PostgreSQL. Ya manejan Esquivel Light, MySQL; son bases de datos que soportan tambi\u00e9n b\u00fasqueda de vectores. ",
            "\u00bfO hay bases de datos especializadas? Pero si ya est\u00e1n usando alguna base de datos de estas no relacionales, les dir\u00eda que simplemente le agreguen este componente de vectores. Si quieren usar una base de datos especializada como Chroma, hay muchas opciones; tambi\u00e9n lo pueden hacer, \u00bfno? Este componente de b\u00fasqueda es la parte importante, ya que pueden hacer b\u00fasquedas de texto normales. Full text search, que tiene sus propios \u00edndices para full text search en MySQL. Lo mismo ocurre con otras bases de datos, o lo pueden combinar con estas b\u00fasquedas de vectores para hacer b\u00fasquedas no tanto basadas en los keywords, sino en el sentido de lo que est\u00e1s buscando en la aplicaci\u00f3n. ",
            "Y eso, entonces, ya ese conocimiento que t\u00fa ten\u00edas en la base de datos se lo vas dando al LLM conforme el usuario lo necesita para poder ir produciendo respuestas. Eso es, a grandes rasgos, c\u00f3mo funciona una soluci\u00f3n RAG. Este es un prompt, uno de los componentes que mencionamos. Tambi\u00e9n estos Lego blocks son los prompts. Este es un producto t\u00edpico para una soluci\u00f3n RAG, \u00bfno? En donde, tal cual, ya hiciste la pregunta del usuario. ",
            "Lleg\u00f3 la pregunta del usuario, hiciste una b\u00fasqueda en la base de datos y te trajiste documentos relevantes que le vas a dar al LLM en el prompt. Entonces, al front, casi casi le pones casi literal el contexto y le pones todos los documentos que te trajiste de la base de datos. Los puedes poner formateados en JSON o en XML, o si es puro texto, pues simplemente copypaste del contexto. Luego, unas instrucciones t\u00edpicas de un programa de RAG son: \"Utiliza este contexto para contestar la pregunta. Si la respuesta no est\u00e1 en el contexto, di que no sabes\". ",
            "Y ah\u00ed est\u00e1s un poquito, tambi\u00e9n evitando las alucinaciones, y lo hago con mi padre de la pregunta, \u00bfno? Y esto es un solo plan que le mand\u00e9 a Salem y ya te va a dar una respuesta basada en tu conocimiento. Entonces, si se fijan, esto es algo en realidad bastante sencillo. "
        ],
        "paragraph_timestamps": [
            1913,
            1955,
            1992,
            2027,
            2048,
            2096,
            2125,
            2161
        ]
    },
    {
        "num_chapter": 10,
        "title": "Data Integration and Structuring Outputs",
        "start_paragraph_number": 71,
        "end_paragraph_number": 80,
        "start_time": 2177,
        "end_time": 2455,
        "paragraphs": [
            "Otro componente que mencionamos son las integraciones. Para hacer estas integraciones, como esta misma, no lo estamos integrando con nuestra base de datos. Hay que pensar en dos cosas. Por una parte, tenemos que estructurar el output que nos genera para poder hacer estas integraciones. Muchas veces necesitamos que la respuesta que venga del LLM, ya junto con el input del usuario, etc\u00e9tera, venga formateada de cierta manera que nosotros la podamos usar en nuestro sistema. ",
            "No nada m\u00e1s para un chat. Un chat es lo primero que pensamos cuando estamos hablando de soluciones aumentadas por LLMs o por inteligencia artificial. Pero no, no nada m\u00e1s con los chats. Si queremos otro tipo de cosas, por ejemplo, podemos extraer informaci\u00f3n de los inputs del usuario, de un texto o de una imagen. ",
            "\u00bfPodr\u00edamos sacar, por ejemplo, de un recibo que nos est\u00e1 dando el usuario? Podemos sacar los line items, cada \u00edtem, cu\u00e1nto cuesta y el total. De una foto que era dif\u00edcil de parchear, el LLM nos ayuda a obtener un JSON con esta informaci\u00f3n. O si tenemos recetas de cocina, un recetario, y lo escaneamos, podemos pedirle a un LLM que nos vaya dando de cada receta el nombre de la receta, una lista de ingredientes en formato JSON y las instrucciones de c\u00f3mo prepararlo. ",
            "Entonces, estamos de contenido no estructurado, sacando contenido estructurado que podemos utilizar de distintas maneras en nuestra aplicaci\u00f3n. O le damos un CSV, un PDF que tiene tablas, a lo mejor informaci\u00f3n financiera, y queremos que esa tabla nos la convierta en un CSV para poderlo usar en nuestra aplicaci\u00f3n de cierta manera. Esto es un uso muy t\u00edpico de este tipo de soluciones, estos modelos en donde de contenido no estructurado estamos sacando contenido ya \u00fatil para nuestras aplicaciones. ",
            "Ya se hizo, nosotros podemos operarlo dentro de nuestro sistema. Podemos acceder a cierto campo y mostrarlo en cierta parte de nuestra interfaz. Entonces, como se hace mucho de esto, puede ser en el mismo prompt. En el mismo prompt, por favor, quiero que des la instrucci\u00f3n de que el output venga de esta manera y le das un ejemplo de c\u00f3mo quieres formateado el output. Esta es una manera, o hay librer\u00edas que te facilitan esto. Tambi\u00e9n OpenAI tiene su formato para hacer estructural, para que te devuelva JSON y lo mandas de cierta manera en su API. ",
            "Lo mismo con Jenny. Tienes algunos ejemplos muy buenos aqu\u00ed, justo para, por ejemplo, extraer las recetas, \u00bfno? \u00bfY c\u00f3mo logras que te mande? As\u00ed es, eso que les comentaba: el nombre de la receta, los ingredientes, las instrucciones. Vienen ejemplos de c\u00f3mo lograrlo. Casi todos los ejemplos en realidad que est\u00e1n mostrando son como este, c\u00f3mo obtener el clima, me parece. No les voy a EH.",
            "Hola, Ma. Si usan la librer\u00eda de Obama o Llama, tambi\u00e9n tiene facilidades para intentar obligar a los QL Tnes que us\u00f3 la AMA para que te manden output estructurado en alg\u00fan formato espec\u00edfico, como JSON. Cuando te vas a esa funcionalidad, hay ciertos modelos que puedo usar y otros que no. Entonces, tambi\u00e9n hay foros en donde dicen: \"Oigan, estoy usando la AMA, pero no me funciona bien con tal modelo para producir JSON\". Ah, es que te recomiendo usar este otro, no esta librer\u00eda que les mencion\u00e9 antes de Instructor. Este es un c\u00f3digo de Instructor, nada m\u00e1s. As\u00ed, muy r\u00e1pido, para ver c\u00f3mo funcionar\u00eda.",
            "Algo as\u00ed est\u00e1n usando un modelo cloud que es de coger, pero si le cambias de aqu\u00ed a en vez de usar coger a usar OpenAI, es el mismo c\u00f3digo. Entonces, simplemente le dices, \"from coger\" o \"from OpenAI\", le mandas el cliente, le mandas tu prompt y le dices qu\u00e9 modelo quieres usar. Le mandas, a lo mejor, un pedazo de texto en donde alguien te est\u00e1 diciendo: \"Ah, pues tengo 25 a\u00f1os y t\u00fa quieres extraer el nombre de la persona y su edad\". Entonces, parchear esto con un retweet, \u00bfs\u00ed? Pues cambia constantemente, \u00bfno? ",
            "Entonces, para nosotros, programarlo tradicionalmente ser\u00eda dif\u00edcil extraer ese tipo de datos. Para LM es muy sencillo, \u00bfno? Entonces, m\u00e1s bien defines en Instructor una clase de los campos que necesitas extraer y es lo que mandas. Respondes, modelas y esto te va a hacer m\u00e1s o menos lo que necesita OpenAI o lo que necesitan distintas librer\u00edas para hacer esa estructura. Al final, ya vamos a tener un objeto que tiene esos campos parcheados."
        ],
        "paragraph_timestamps": [
            2177,
            2208,
            2228,
            2259,
            2291,
            2331,
            2355,
            2391,
            2424
        ]
    },
    {
        "num_chapter": 11,
        "title": "Integrating Functions with Language Models",
        "start_paragraph_number": 80,
        "end_paragraph_number": 86,
        "start_time": 2455,
        "end_time": 2626,
        "paragraphs": [
            "Este es un ejemplo muy peque\u00f1o usando Instructor, no para hacer todo, sino para hacer integraciones. Es una de las cosas que necesitas. Otra muy importante es funci\u00f3n Collins, o Tool, que es otro nombre con el que se le conoce a estas funcionalidades. En el ambiente OpenAI, Cloud Mining o Llama, tambi\u00e9n tienen tus PS para c\u00f3mo hacer que los LM mismos. ",
            "Ac\u00e1 nosotros estamos m\u00e1s bien de la respuesta del LM. La vamos a usar para nosotros insertar algo en la base de datos, pero nosotros en nuestro c\u00f3digo que escribimos no. Mientras que en funci\u00f3n Collins le vamos a decir al LM qu\u00e9 funciones tiene disponibles para que \u00e9l decida si las quiere llamar o no, en base a las instrucciones del usuario. Entonces, es un uso un poco diferente, \u00bfno? Este es m\u00e1s reactivo y no depende de nosotros si se va a llamar la funci\u00f3n.",
            "Los ejemplos tambi\u00e9n est\u00e1n padres. Casi siempre, casi todos estos ejemplos son acerca del clima, de que el LM responde de acuerdo a las preguntas del usuario. Si tiene que, si tiene una funci\u00f3n para obtener el clima actual, este provee al usuario, \u00bfno? Bien, las instrucciones. Vamos a ver otra vez un ejemplo muy peque\u00f1o con esta otra librer\u00eda que les mencionaba, que se llama E-Suite, que te ayuda a usar, te abstrae un poco la dificultad de hacer esto.",
            "Funci\u00f3n Collins, desde estos elementos, es un ejemplo chiquito, \u00bfno? Entonces, defines tu funci\u00f3n \"Willy Trains\" y necesitas mandar la ubicaci\u00f3n, \u00bfno? De en d\u00f3nde est\u00e1s y la hora del d\u00eda. En base a eso, t\u00fa ya checas alg\u00fan servicio externo o algo que tengas por ah\u00ed para devolver una respuesta. ",
            "Por ejemplo, en este caso, esta es la funci\u00f3n que quieres que el LM pueda llamar. Tienes los inputs del usuario que llegan y, por ejemplo, aqu\u00ed hay un usuario que est\u00e1 diciendo en su input: \"Yo vivo en San Francisco, \u00bfpudieras checar el clima? Planeo hacer un picnic alrededor de las dos\". Entonces, a la hora de que usas esta librer\u00eda, simplemente le mandas las herramientas disponibles, las funciones que va a tener el LM para decidir si la va a llamar o no. Le agregas un par\u00e1metro de cu\u00e1ntas veces puede iterar en estas llamadas para mandar la respuesta final al usuario y listo.",
            "El LM solito va a llamar tu funci\u00f3n y va a crear una respuesta con el output de la funci\u00f3n para responder la pregunta del usuario. Otro ejemplo muy t\u00edpico es llamar funciones externas que el LM pueda llamar para checar el clima, checar tu base de datos, insertar nuevos r\u00e9cords en la base de datos, leer r\u00e9cords de la base de datos, etc\u00e9tera."
        ],
        "paragraph_timestamps": [
            2455,
            2481,
            2513,
            2540,
            2565,
            2599
        ]
    },
    {
        "num_chapter": 12,
        "title": "Understanding Agents in AI",
        "start_paragraph_number": 86,
        "end_paragraph_number": 90,
        "start_time": 2626,
        "end_time": 2770,
        "paragraphs": [
            "Y eso nos lleva un poquito a este buzzword que est\u00e1 muy de moda hoy en d\u00eda, que es \"agentes\". Vamos a hablar un poco de agentes. ",
            "Hubo, esta semana, no, la semana pasada, una conferencia en Nueva York que era el \"Engineering Summit\", donde, por ejemplo, justo Adri\u00e1n habl\u00f3 mucho de agentes. Alguien defin\u00eda a los agentes; \u00e9l es uno de los organizadores de la conferencia. Dec\u00eda que normalmente un agente consiste en tener tu modelo equipado con instrucciones que puede dar el usuario sobre lo que quiere hacer. Hay que definir qu\u00e9 cosas s\u00ed puede hacer y qu\u00e9 cosas no. Adem\u00e1s, hay herramientas o funciones que puede usar para extender las capacidades de este modelo que corre en un runtime. ",
            "Esta definici\u00f3n est\u00e1 un poco rebuscada, pero es \u00fatil. Despu\u00e9s sali\u00f3 una definici\u00f3n mucho m\u00e1s sencilla, que dir\u00eda que es: un agente es simplemente un modelo que tiene acceso a funciones y herramientas, y que est\u00e1 corriendo en un loop, en un \"while true\". Este modelo tiene acceso a ciertas herramientas y las puede usar si lo necesita. Por ejemplo, el agente que liber\u00f3 Anthropic, y t\u00fa puedes ver el c\u00f3digo fuente, es bastante m\u00e1s completo. Te dice c\u00f3mo funciona esta herramienta de Anthropic que todo el mundo am\u00f3, c\u00f3mo te ayuda a hacer c\u00f3digo, y t\u00fa le puedes leer el c\u00f3digo fuente, te lee todo tu directorio, te puede modificar algunas cosas, agregar comentarios a tu repositorio de Git, etc\u00e9tera. ",
            "Tiene muchos tools disponibles, como un \"dispatch agent\", que puede correr cosas en el backend, leer archivos de tu l\u00ednea de comandos, editarlos, reemplazar peque\u00f1as cosas, etc\u00e9tera. No tiene muchas herramientas disponibles que simplemente est\u00e1n corriendo en un loop dentro de su c\u00f3digo. No es realmente un \"while true\"; se ejecuta esto y no es m\u00e1s que eso, no tiene un servidor externo. En resumen, un agente es un modelo con acceso a funciones que corre en un loop. "
        ],
        "paragraph_timestamps": [
            2626,
            2634,
            2677,
            2733
        ]
    },
    {
        "num_chapter": 13,
        "title": "Testing and Validation in AI Solutions",
        "start_paragraph_number": 90,
        "end_paragraph_number": 98,
        "start_time": 2770,
        "end_time": 3008,
        "paragraphs": [
            "Esto es para intentar desmitificar esa palabra. Tiene mucho buzz hoy en d\u00eda, pero no es m\u00e1s que eso. \u00bfYa pueden construir ustedes sus propios agentes? Bueno, esto va a ser muy importante, tambi\u00e9n la parte de c\u00f3mo probar, ya que les est\u00e1n ense\u00f1ando a ustedes todo acerca de \"unit testing\", \"functional testing\" y \"automation\". Cuando haces soluciones con inteligencia artificial, tambi\u00e9n vas a necesitar probarlas. ",
            "Algunas recomendaciones: obviamente, estas pruebas deben estar corriendo de manera autom\u00e1tica, con integraci\u00f3n continua, mientras est\u00e1n haciendo el c\u00f3digo, para que constantemente lo est\u00e9n evolucionando. Si cambian el modelo, por ejemplo, si estaban usando Claude 3.5 y sali\u00f3 el 3.7, se actualizan. Seguramente algunas pruebas se van a romper porque est\u00e1 cambiando el modelo que est\u00e1s corriendo por detr\u00e1s. Entonces, correrlas en un loop continuo con tus pruebas va a ser s\u00faper importante. ",
            "Va a ser muy importante involucrar a \"subject matter experts\", es el t\u00e9rmino, a gente que entienda muy bien el conocimiento de tu aplicaci\u00f3n, de qu\u00e9 se trata, qu\u00e9 tipo de conocimiento es importante, qu\u00e9 tipo de respuestas son las que esperamos y que sean ejemplos realmente significativos para que esos ejemplos de uso real de tu aplicaci\u00f3n los puedas probar. Es importante tener un set de pruebas que sea significativo. ",
            "Normalmente es muy famosa esta frase cuando se habla de datos en sistemas: \"si le metes basura al sistema, te va a sacar basura\". Entonces, es importante que estas pruebas sean buenas. Normalmente vas a estar checando la entrada del sistema de informaci\u00f3n. Si est\u00e1s usando alg\u00fan sistema para traer informaci\u00f3n de tu base de datos para responderle al usuario sobre alguna cosa, algo que puedas probar sin siquiera irte a la parte de LM son tus mecanismos de b\u00fasqueda que te est\u00e9n regresando los registros de la base de datos que t\u00fa esperas que te regresen en los primeros lugares. ",
            "Es muy sencillo: simplemente haz pruebas de integraci\u00f3n de tu base de datos. Cuando llega cierto input del usuario y t\u00fa buscas en la base de datos, esperas que el resultado top sea este, porque sabes que ese es el documento relevante. Ah\u00ed es donde es muy importante el experto de tu base de datos, que sepa realmente cu\u00e1les son las respuestas relevantes que est\u00e1s esperando. Eso es muy f\u00e1cil de probar; es simplemente un query en la base de datos. ",
            "\u00bfY cu\u00e1les son los resultados que est\u00e1s esperando? Conforme a eso, probablemente las pruebas las vas a tener que cambiar cuando cambien los queries de datos, pero te van a dar una idea de si tu sistema se est\u00e1 rompiendo desde el input de conocimiento que le est\u00e1s mandando. ",
            "Otro tip es que tambi\u00e9n el output del modelo, es decir, las respuestas que generas, tienen que ser revisadas. Si se fijan, hay este argumento de temperatura, que te dicen que si le pones una temperatura alta, el modelo es m\u00e1s creativo. Lo que est\u00e1 haciendo es agregarle un elemento aleatorio a las respuestas que genera. Si t\u00fa pones esa temperatura en cero, le quitas el elemento aleatorio y a un input te debe devolver siempre el mismo output. ",
            "Entonces, en tus pruebas del contenido que esperas generar con el modelo, pones la temperatura en cero, env\u00edas un input del usuario y vas a esperar cierta respuesta, \u00bfno? Esa puede ser una prueba de integraci\u00f3n muy buena y certera. Si modificas algo en tu sistema y se rompe, vas a ver que algo pas\u00f3 y necesitas modificar algo en el c\u00f3digo."
        ],
        "paragraph_timestamps": [
            2770,
            2801,
            2836,
            2869,
            2907,
            2938,
            2952,
            2986
        ]
    },
    {
        "num_chapter": 14,
        "title": "Exploring Specialized AI Models",
        "start_paragraph_number": 98,
        "end_paragraph_number": 104,
        "start_time": 3008,
        "end_time": 3195,
        "paragraphs": [
            "Y bueno, ya casi se nos acaba el tiempo, pero tambi\u00e9n creo que es muy importante aprender que est\u00e1bamos hablando de modelos de lenguaje, pero hay muchos otros modelos que vale la pena considerar cuando estamos empezando a hacer soluciones aumentadas con inteligencia artificial. La inteligencia artificial no solamente son estos modelos de lenguaje; hay muchos modelos. ",
            "Voy a mencionar algunos, por ejemplo, modelos m\u00e1s especializados. Unos son los modelos de embedding que se usan justamente en estas soluciones. Hay de muchos tipos, algunos son solo para ingl\u00e9s o para chino. Hay otros modelos de embedding que son multiling\u00fces o solo de franc\u00e9s, o creo que solo de espa\u00f1ol. No he visto muchos de esos modelos que te pueden sacar tambi\u00e9n cu\u00e1l es el vector que define una imagen, no solo un texto, para poder buscar im\u00e1genes o videos. Tambi\u00e9n hay modelos de embedding que te ayudan a sacar el vector de medios, de una imagen, de un video, de un sonido. ",
            "As\u00ed podemos, desde nuestra base de datos, traernos el contenido relevante. Para eso sirven estos modelos de embedding. Hablando de todos estos ejemplos, la gran diferencia es que son modelos especializados y mucho m\u00e1s peque\u00f1os que corren de manera m\u00e1s eficiente. Normalmente, los modelos de lenguaje, incluso los de c\u00f3digo abierto, van a necesitar un servidor potente, probablemente, si quieres respuestas de calidad. Pero estos modelos especializados, por ser m\u00e1s peque\u00f1os, pueden correr en hardware m\u00e1s limitado. ",
            "Hay algunos de estos modelos de embedding, por ejemplo, que corren en mi Raspberry Pi. Entonces, t\u00fa puedes tener tu sistema en producci\u00f3n, con la base de datos de vectores ya indexada. Cuando llega una pregunta del usuario, todo est\u00e1 corriendo en la Raspberry Pi, que est\u00e1 tomando la pregunta del usuario, convirti\u00e9ndola en vectores para comparar y buscar cu\u00e1les vectores son los m\u00e1s cercanos a esa pregunta. Luego, generas la respuesta con un modelo que est\u00e1 hospedado en otro lado, no en la Raspberry Pi, pero el modelo de embedding s\u00ed puede correr en ella.",
            "Otros modelos, como los modelos de visi\u00f3n, te permiten extraer informaci\u00f3n de im\u00e1genes. Claro que puedes usar modelos grandes para eso, pero hay otros modelos m\u00e1s peque\u00f1os, como Mondrian o Small Vision Language Model, que tambi\u00e9n te permiten hacer este tipo de cosas, como la detecci\u00f3n de un coche en un video, extrayendo marca y modelo. Esto puede estar corriendo con un modelo peque\u00f1o, en lugar de un modelo enorme de 600 millones de par\u00e1metros; estos modelos son de 2 billones de par\u00e1metros y saben correr en dispositivos mucho m\u00e1s peque\u00f1os. ",
            "Entonces, vale mucho la pena verlos. Puedes describir im\u00e1genes, detectar objetos, extraer informaci\u00f3n. Por ejemplo, extraer de un recibo los precios individuales y el total lo puedes hacer con modelos grandes, pero tambi\u00e9n con estos modelos peque\u00f1os y gratuitos en tu computadora o en tu tel\u00e9fono. "
        ],
        "paragraph_timestamps": [
            3008,
            3029,
            3068,
            3096,
            3132,
            3174
        ]
    },
    {
        "num_chapter": 15,
        "title": "Applications of AI in Text and Speech Processing",
        "start_paragraph_number": 104,
        "end_paragraph_number": 108,
        "start_time": 3195,
        "end_time": 3317,
        "paragraphs": [
            "Otros modelos especializados son para extracci\u00f3n de texto, como el OCR, que era el reconocimiento \u00f3ptico de caracteres. Convertir un PDF a texto. Hay dos proyectos de c\u00f3digo abierto que se usan en otros modelos, como IBM Markup, que si t\u00fa le das un PDF, te van a extraer todo el texto, incluso de fotos de documentos, y lo van a formatear en texto. ",
            "Tambi\u00e9n hay modelos de speech to text. Por ejemplo, Whisper, que es un modelo de c\u00f3digo abierto que puedes usar para escuchar audio y traducirlo a texto. Esto puede ser muy \u00fatil. Justo hicimos unas aplicaciones hace algunos semestres, donde los alumnos ayudaban a practicar ingl\u00e9s. Entonces, t\u00fa le hablabas a la computadora, extra\u00eda el texto, y ese texto despu\u00e9s lo evaluaban con algunos problemas usando un modelo de lenguaje que te dec\u00eda si estaba bien o mal. ",
            "La calificaci\u00f3n te iba evaluando tu nivel de ingl\u00e9s o te ayudaba a practicar tu ingl\u00e9s. No, y eso es con el modelo speech to text. Por ejemplo, hay un Whisper web, una versi\u00f3n de Whisper que corre en el navegador. Entonces, t\u00fa puedes directamente en el navegador, sin usar un servidor. Es un modelo m\u00e1s peque\u00f1o que puede correr en el browser. ",
            "\u00bfDepende de la computadora y de qu\u00e9 browser, verdad? Y te puede ir extrayendo en el mismo navegador este texto de audios o de archivos, o de la c\u00e1mara del video. Tambi\u00e9n. Este es un ejemplo de Mondrian, Bustamante extrayendo, y esto de c\u00f3mo se usan estos modelos. Es c\u00f3digo muy sencillo. "
        ],
        "paragraph_timestamps": [
            3195,
            3241,
            3271,
            3292
        ]
    },
    {
        "num_chapter": 16,
        "title": "Creating Proofs of Concept with Python Libraries",
        "start_paragraph_number": 108,
        "end_paragraph_number": 110,
        "start_time": 3317,
        "end_time": 3373,
        "paragraphs": [
            "Finalmente, esto te va a ayudar a crear pruebas de concepto. Hay librer\u00edas en Python muy sencillas para esto. Streamline, Grader\u00edo, Marimo. Aqu\u00ed son excelentes librer\u00edas de prototipado en Python. Fast HTML tambi\u00e9n es muy bueno si quieres un poco m\u00e1s de personalizaci\u00f3n. Incluso aqu\u00ed hay un c\u00f3digo open source hecho por m\u00ed, no hay promocion\u00e1ndolo, que es justo toda una aplicaci\u00f3n sencilla de c\u00f3mo extraer texto, hacer b\u00fasquedas y luego crear un chat muy sencillo con Fast HTML. ",
            "Hasta ah\u00ed no est\u00e1 s\u00faper avanzado, pero es un ejemplo completo de c\u00f3mo hacer esto en Python. En JavaScript tambi\u00e9n se puede hacer mucho de esto y hay algunos workshops por ah\u00ed de c\u00f3mo funcionan los embeddings. Hay un video que grab\u00e9 hace poquito, un repositorio y unos slides. "
        ],
        "paragraph_timestamps": [
            3317,
            3354
        ]
    },
    {
        "num_chapter": 17,
        "title": "Ethical Considerations in AI Development",
        "start_paragraph_number": 110,
        "end_paragraph_number": 117,
        "start_time": 3373,
        "end_time": 3579,
        "paragraphs": [
            "Ah, muy importante, consideraciones \u00e9ticas. Ustedes que son privilegiados, no simplemente por estudiar en la universidad, digamos en el TEC de Monterrey, y de aprender este tipo de tecnolog\u00edas, no podemos dejar de mencionar qu\u00e9 es lo que queremos hacer con la inteligencia artificial. Tal cual, como en Spiderman, con gran poder, conlleva gran responsabilidad. Es incre\u00edble lo c\u00ednicos que son algunas personas. ",
            "Ahorita que estoy en este espacio, hay quienes dicen que con esto vamos a poder eliminar o cambiar a todo nuestro call center y que sean simplemente algunos agentes. Entonces, nosotros que estamos permitiendo el desarrollo de este tipo de soluciones, \u00bfcu\u00e1l es nuestro objetivo? \u00bfEs realmente reducir la fuerza laboral, ahorrarnos dinero y ganarlo nosotros mismos? \u00bfO realmente estamos buscando empoderar, ayudar a que m\u00e1s gente tenga acceso a estas tecnolog\u00edas para beneficio de todos? ",
            "Los invito a que, conforme aprendan de estas tecnolog\u00edas, intenten compartir este conocimiento no solo con sus compa\u00f1eros, sino tambi\u00e9n con sus pap\u00e1s, con sus vecinos, sobre c\u00f3mo se usan estas herramientas. Siempre tengan en cuenta para qui\u00e9n est\u00e1n creando estas soluciones. No es para el beneficio de unos pocos, sino de la manera l\u00f3gica y no solo para ustedes o el cliente de Alcatraz. ",
            "Es muy importante porque es algo muy nuevo que viene a revolucionar nuestra industria. Cuanto menos c\u00ednicos seamos con el tipo de soluciones que desarrollamos, mejor. Hay algunos slides que hablan un poco m\u00e1s sobre otros problemas \u00e9ticos que est\u00e1n imbuidos en los sistemas de lenguaje. ",
            "\u00bfTienen ciertas tendencias o prejuicios? Casi se pueden mapear. Este modelo es un poco m\u00e1s libertario, es m\u00e1s autoritario, est\u00e1 m\u00e1s alineado a la derecha o a la izquierda. Los modelos ya traen esos prejuicios. Ya hay ejemplos de las preguntas. Hay papeles completos que vienen desde los datos de entrenamiento o a veces simplemente en el fine-tuning. ",
            "Eran muy famosos ejemplos de los DEC. Si le preguntaban sobre Tiananmen, no te respond\u00eda, y no es porque los datos no est\u00e9n ah\u00ed. Luego pod\u00edan hacer trucos de que, ah, resp\u00f3ndeme, pero reemplazando las \"a\" por \"1\" y las \"6\" por \"1\". Y s\u00ed, respond\u00eda. \u00bfQu\u00e9 pas\u00f3 en Tiananmen? Lo mismo pasa con los modelos americanos. Si t\u00fa le preguntas a muchos modelos sobre Donald Trump o sobre alg\u00fan problema pol\u00edtico actual, muy probablemente te da una respuesta muy vaga. ",
            "Muchas veces no sabemos cu\u00e1l es el resultado \u00f3ptimo. Si queremos que nuestros modelos reflejen fielmente la realidad, que en nuestra realidad t\u00edpica siempre hay desigualdades, o si queremos que nuestros modelos de lenguaje sean un reflejo del futuro ideal que estamos buscando. Eso muchas veces nos termina disparando en el pie, como esos ejemplos rid\u00edculos que hab\u00eda de Gemini que te mostraba cuando le ped\u00edas que te dibujara a los founding fathers de Estados Unidos y todos eran racialmente diversos. "
        ],
        "paragraph_timestamps": [
            3373,
            3401,
            3437,
            3464,
            3483,
            3510,
            3539
        ]
    },
    {
        "num_chapter": 18,
        "title": "Community Engagement and Open Source Tools",
        "start_paragraph_number": 117,
        "end_paragraph_number": 119,
        "start_time": 3579,
        "end_time": 3633,
        "paragraphs": [
            "Es dif\u00edcil encontrar el balance. Bueno, tenemos una comunidad. Tambi\u00e9n los invito a que, si quieren participar, estamos construyendo algunas herramientas open source del RAC para el Diario Oficial de la Federaci\u00f3n. Estamos ahorita penetrando ese proyecto.",
            "Algunos ejemplos de c\u00f3digo que les mostr\u00e9, un canal de Discord que apenas est\u00e1 empezando, entonces en realidad est\u00e1 ahorita muy callado, pero hay otros por fuera. Por ejemplo, hasta este de Jogging FACE tiene un canal de Discord, y ese s\u00ed, con much\u00edsima gente que est\u00e1 interactuando sobre c\u00f3mo construir soluciones con Open Source e inteligencia artificial. Este s\u00ed busca hogar, Johnny FACE, Discord."
        ],
        "paragraph_timestamps": [
            3579,
            3592
        ]
    },
    {
        "num_chapter": 19,
        "title": "Reflections and Questions",
        "start_paragraph_number": 119,
        "end_paragraph_number": 130,
        "start_time": 3633,
        "end_time": 3925,
        "paragraphs": [
            "Creo que ser\u00eda todo por ahorita. Si tienen preguntas, esperen un momento. A ver si me cambio, me cambio tambi\u00e9n. Ah\u00ed, \u00bfme escuchas mejor? S\u00ed, o K. Estuvo este Joaqu\u00edn, pues estuvo padr\u00edsimo, la verdad. O sea, demasiada informaci\u00f3n. La forma en que la concentraste toda se me hizo muy, muy padre.",
            "Yo creo que es un video que grabamos para que despu\u00e9s lo puedan volver a ver. S\u00e9 que ahora llevan trabajado con ella en clases anteriores o por su cuenta. No s\u00e9, ahorita ya est\u00e1n pensando c\u00f3mo lo van a integrar. Creo que fue un gran momento tener ahorita la sanci\u00f3n para que empiecen a ver d\u00f3nde m\u00e1s o c\u00f3mo lo pueden integrar, qu\u00e9 opciones hay.",
            "Yo s\u00ed quiero abrir un poco, si tienes tiempo, Joaqu\u00edn, para preguntas. Si tienes alguna pregunta o comentario, no s\u00e9, creo que si tienes unos minutos, estar\u00eda muy padre escuchar a los alumnos y ver sus impresiones. Tambi\u00e9n a todos los que est\u00e1n aqu\u00ed presentes, a los que estamos conectados.",
            "A ver si adelante, Felipe, si quieres hacer tu pregunta. Porque el micr\u00f3fono del sal\u00f3n no funciona. No importa. Silva, \u00bfqu\u00e9 piensas que va a ser el futuro? Por ejemplo, la cantes una hace un a\u00f1o era como una tecnolog\u00eda super nueva. \u00bfQu\u00e9 veo ahora que puede ser el pr\u00f3ximo? ",
            "Pues mira, por un lado, en la parte de RAC, hoy en d\u00eda todo el mundo habla de agentes. Los estamos desmitificando un poco en la presentaci\u00f3n en Rack. Hay muchas mejoras que vienen. Una de ellas es que, ahorita, nada m\u00e1s Yamile, pero Jenny tiene este tama\u00f1o de contexto de 2,000,000 de caracteres, de tokens. Entonces, muchas veces ya ni necesitas una base de datos; le puedes mandar as\u00ed toda la informaci\u00f3n y que te d\u00e9 la respuesta.",
            "S\u00e9 que por ah\u00ed realmente no es lo m\u00e1s factible, pero hoy, porque nada m\u00e1s Jenny tiene eso y alg\u00fan otro modelo Open Source que no es f\u00e1cil de correr, pero para all\u00e1 va. Yo creo que hacia adelante van a seguir aumentando estos tama\u00f1os de contexto. Les dir\u00eda que m\u00e1s bien ustedes pongan mucha atenci\u00f3n a c\u00f3mo funciona por detr\u00e1s. El problema va a seguir siendo el mismo: c\u00f3mo traer informaci\u00f3n relevante, aunque t\u00fa le puedas pegar 2,000,000 de tokens.",
            "\u00bfVa a ser importante cuando tienes millones de PDF con informaci\u00f3n? No la puedes alimentar toda de un jal\u00f3n. Justo ahorita que estamos con lo del Diario Oficial de la Federaci\u00f3n, una sola edici\u00f3n del Diario Oficial es como medio mill\u00f3n de tokens. Entonces, con dos ediciones, con cuatro ediciones, y sale una edici\u00f3n cada d\u00eda, ya te llenaste el tama\u00f1o de contexto. ",
            "La soluci\u00f3n es RAC. Las han querido matar muchas veces, pero van a seguir siendo relevantes por un buen rato. Entonces, simplemente s\u00ed, aprendan muy bien por la parte de agentes. Esta parte, Tully de funci\u00f3n Colin, y por el lado de Raz, entiendan bien c\u00f3mo funciona por detr\u00e1s, porque ah\u00ed hay muchas mejoras en Rack. Tambi\u00e9n se habla mucho de c\u00f3mo mejoran los LSY, c\u00f3mo est\u00e1n evolucionando constantemente los modelos RAC. ",
            "El que yo estaba usando hace un a\u00f1o, hoy en d\u00eda ya sacaron tres versiones nuevas, sacaron Modern Bird, y entonces ah\u00ed creo que todav\u00eda es un campo que no ha explotado. No hay modelos de EADS, por ejemplo, nada m\u00e1s de espa\u00f1ol, que podr\u00edan ser m\u00e1s chiquitos y correr todav\u00eda m\u00e1s eficientemente. Casi hay nada m\u00e1s, casi casi, o en ingl\u00e9s, o en chino, o multilenguaje, que son bien grandes. ",
            "Si no tienen tan buena eficiencia, hacer fine-tuning de un modelo de embedding tampoco es complicado, y muy poca gente lo hace. Eso s\u00ed puede correr en mi computadora. Un fine-tuning de un modelo en vez de ingl\u00e9s, para que sea todav\u00eda m\u00e1s eficiente o para que sea nada m\u00e1s para espa\u00f1ol, por ejemplo, y tener un modelo chiquito que funcione para espa\u00f1ol. ",
            "Entonces, en esa parte de Racks, s\u00ed hay mucho, mucha blogspot, mucho de d\u00f3nde cortar con ganancias que a futuro vienen, como esas que les comento, fine-tuning de modelo, RAC y modelos m\u00e1s especializados. "
        ],
        "paragraph_timestamps": [
            3633,
            3668,
            3695,
            3720,
            3745,
            3774,
            3807,
            3831,
            3861,
            3884,
            3904
        ]
    },
    {
        "num_chapter": 20,
        "title": "Insights on AI Models and Future Directions",
        "start_paragraph_number": 130,
        "end_paragraph_number": 137,
        "start_time": 3925,
        "end_time": 4157,
        "paragraphs": [
            "\u00bfAlguna otra pregunta o comentario? Veo que hay mucho inter\u00e9s. \u00bfCon qu\u00e9 se quedan hoy? \u00bfCon qu\u00e9 de la pl\u00e1tica los dej\u00f3 pensando? \u00bfQu\u00e9 aprendieron o con qu\u00e9 se quedan? \u00bfAlguien que quiera compartir?",
            "Yo dir\u00eda que ha habido muchas cosas interesantes. Bueno, mensaje a ver si puedes. \u00bfTambi\u00e9n pregunto alguna vez o K? Hola Joaqu\u00edn, \u00bfqu\u00e9 tal? Muchas gracias. Bueno, yo creo que es muy interesante. Creo que nos habl\u00f3 un poquito sobre las bases que de repente vemos mucho en Internet, pero que no logramos comprender completamente. ",
            "Por ejemplo, eso de Rack, todos los nuevos modelos que est\u00e1n saliendo. Creo que si yo me llevara algo, ser\u00eda que existen muchos nuevos modelos que salen muy r\u00e1pido y que parece que nunca dejan de avanzar. Pero yo te dir\u00eda, \u00bfcu\u00e1l es tu opini\u00f3n? Justamente, en esa \u00e1rea donde buscamos la innovaci\u00f3n y buscamos como decorar los modelos existentes, \u00bfpodemos menospreciar el \u00e1rea de la seguridad en la inteligencia artificial? No, como que la inteligencia artificial que desarrollamos se asegura. Por ejemplo, he escuchado que en Europa dicen que est\u00e1n retrasando mucho el desarrollo de nuevos modelos precisamente por tener mucho control sobre esos modelos. ",
            "Entonces, no s\u00e9 cu\u00e1l es tu opini\u00f3n sobre eso. Yo dir\u00eda que primero es importante aprender c\u00f3mo funcionan por detr\u00e1s. A m\u00ed todo este tema de modelos Open Source y dejar de depender de los grandes proveedores, entender c\u00f3mo funciona un sistema RAC, c\u00f3mo funcionan otros modelos peque\u00f1os de lenguaje y estarlos utilizando, nos va a habilitar a depender menos de ellos y a entender mejor c\u00f3mo asegurar la calidad de las respuestas. ",
            "Si siempre estamos usando lo que los grandes proveedores nos dan, creo que vamos a perder justo ah\u00ed, en el entendimiento de c\u00f3mo funcionan esos sistemas. S\u00e9 que, por ejemplo, hay sistemas que te facilitan hacer una integraci\u00f3n RAC, y t\u00fa nada m\u00e1s le mandas tus documentos y le mandas tu query a trav\u00e9s de una llamada. Te devuelve los resultados, pero ah\u00ed tambi\u00e9n te est\u00e1s perdiendo de ciertas cosas, como hacer el chonqing, de c\u00f3mo hacer los queries, de qu\u00e9 modelos hay disponibles para hacer estos en Bearings. ",
            "Entre mejor entiendan las bases de c\u00f3mo funcionan estos modelos, van a poder asegurar sus sistemas. Van a tener menos dependencia y m\u00e1s libertad. Luego de ir cambiando algunos componentes, los invitar\u00eda a leer mucho la documentaci\u00f3n para estar al tanto de muchas de estas cosas nuevas. Con\u00e9ctense con la comunidad, ya sea con esta de Joaqu\u00edn en Twitter. S\u00e9 que est\u00e1 muerto para muchos, pero en el \u00e1mbito de inteligencia artificial est\u00e1 m\u00e1s vivo que nunca. Todos los grandes investigadores y muchos de los grandes laboratorios de inteligencia artificial tienen sus cuentas de Twitter, y las personas que lo est\u00e1n desarrollando tambi\u00e9n las tienen y est\u00e1n todos los d\u00edas. ",
            "\u00bfHay algo nuevo? A veces te explota la cabeza, pero est\u00e1 padre. "
        ],
        "paragraph_timestamps": [
            3925,
            3948,
            3967,
            4010,
            4058,
            4104,
            4153
        ]
    },
    {
        "num_chapter": 21,
        "title": "Conclusion and Farewell",
        "start_paragraph_number": 137,
        "end_paragraph_number": 142,
        "start_time": 4157,
        "end_time": 4237,
        "paragraphs": [
            "Bueno, perfecto. Muchas gracias, Joaqu\u00edn. Un \u00faltimo comentario, y si no, ya despedimos a Joaqu\u00edn. \u00bfAlguien m\u00e1s quiere preguntar algo o ya despedimos a nuestro invitado? No, que es muy interesante la pl\u00e1tica, Joaqu\u00edn. Bueno, pues much\u00edsimas gracias de vuelta. Es un gusto poder escucharte y que hayas estado en este espacio. Vamos a ir cont\u00e1ndote c\u00f3mo va todo esto por ac\u00e1. ",
            "Gracias a \u00c1lvaro tambi\u00e9n por conectarse. \u00c1lvaro, no s\u00e9 si quieres comentar algo, preguntar algo. No, de momento no. Digo, fue bastante interesante el tema que abord\u00f3 Joaqu\u00edn. Para m\u00ed, fue muchas cosas nuevas, de verdad, muy interesante. ",
            "S\u00ed, muchas gracias por este resumen de informaci\u00f3n y parte global de c\u00f3mo se va moviendo la inteligencia artificial. Pues nada, muchas gracias de vuelta. Gracias a \u00c1lvaro. Nos vemos al ratito, a las 11, para que veas las presentaciones de los alumnos. Salen las propuestas. ",
            "Gracias, Joaqu\u00edn. Nos vemos. \u00a1Bye! Y ah\u00ed tienes el link para que lo compartas despu\u00e9s con los alumnos de la presentaci\u00f3n. Ah, s\u00ed, les paso la presentaci\u00f3n. Ahorita se las subo en Teams y subo el video en Teams tambi\u00e9n y te lo paso de vuelta. ",
            "\u00a1Dale, gracias! Nos vemos. \u00a1Bye, bye!"
        ],
        "paragraph_timestamps": [
            4157,
            4189,
            4205,
            4223,
            4237
        ]
    }
]